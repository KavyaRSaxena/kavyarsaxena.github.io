<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kavyarsaxena.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kavyarsaxena.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-30T11:16:03+00:00</updated><id>https://kavyarsaxena.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding True Class Probability for Reliable Predictions</title><link href="https://kavyarsaxena.github.io/blog/2026/tcp/" rel="alternate" type="text/html" title="Understanding True Class Probability for Reliable Predictions"/><published>2026-01-01T00:00:00+00:00</published><updated>2026-01-01T00:00:00+00:00</updated><id>https://kavyarsaxena.github.io/blog/2026/tcp</id><content type="html" xml:base="https://kavyarsaxena.github.io/blog/2026/tcp/"><![CDATA[<p>Confidence criteria is essential for identifying ambiguous or hard-to-classify samples within the same domain. Samples with low confidence often lie near decision boundaries, contain noise, or exhibit features that are weakly discriminative. By contrast, high-confidence predictions typically correspond to samples that are well supported by the learned representation. Distinguishing between these cases enables more informed decision-making in downstream tasks such as active learning. Among various confidence measures, the true class probability is one of the most intuitive and widely used. It directly reflects how strongly the model believes in the correct label under its learned probability distribution.</p> <h3 id="1-pre-training">1. Pre-training</h3> <p>Before understanding how TCP works, we need to have a pre-trained model. For simplicity, consider an image classification problem where the input is classified into 5 classes.</p> <p>Consider a dataset \(D_1^S = \{(X_i,Y_i)\}_{i=1}^{I}\), where \(X_i\) is an \(N\times N\) image and \(Y_i\in \{0,1\}^C\) is a one-hot vector over \(C=5\) classes. The ground truth class \(y^*_i\) is simply obtained by using argmax over the one-hot vector \(Y_i\). We train the base model \(f_{[\phi,\theta]}\) on \(D_1^S\) dataset. Here, \(\phi\) and \(\theta\) are the parameters of the feature extractor layers and classifier layer, respectively which are randomly initialized. With \(X_i\) as the input, the base model \(f_{[\phi,\theta]}\) predicts an output distribution \(\hat{Y}_i\) of dimension \(C\) by computing softmax output for each of the \(C\) classes. During training, the base model parameters \(\phi\) and \(\theta\) are updated using the gradient descent algorithm as:</p> \[[\phi,\theta] \leftarrow [\phi,\theta] - \alpha \nabla_{[\phi,\theta]}\mathcal{L}(f_{[\phi,\theta]})\] <p>where \(\alpha \in \mathbb{R}^+\) is the learning rate and loss \(\mathcal{L}\) is defined as the categorical cross entropy loss. The base model \(f_{[\phi,\theta]}\) is trained for \(E\) epochs.</p> <h3 id="2-output-probabilities">2. Output Probabilities</h3> <p>With the pre-trained base model \(f_{[\phi,\theta]}\), we now examine different types of output probabilities.</p> <p>Consider a sample \((X_i,Y_i)\in D_1^S\). With \(X_i\) as an input, the pre-trained base model \(f_{[\phi,\theta]}\) predicts an output distribution \(\hat{Y}_i\).</p> <h4 id="21-maximum-class-probability">2.1 Maximum Class Probability</h4> <p>A standard confidence criteria to obtain confidence value is to consider the softmax probability corresponding to the predicted class. This is also termed as Maximum Class Probability (MCP). The MCP value is obtained by</p> \[\underset{c}{max} \, P(\hat{Y}_i = c|[\phi,\theta],X_i) = P(\hat{Y}_i = \hat{y}_i|[\phi,\theta],X_i)\] <p>This is depicted in the following figure.</p> <figure style="text-align:center;"> <img src="/assets/img/blogs/tcp/mcp.png" width="450"/> </figure> <p><strong>Drawback:</strong> It leads to high confidence values for both correct and incorrect predictions as shown below. Ideally, the model should give low confidence for incorrect predictions and high confidence for correct predictions.</p> <figure style="text-align:center;"> <img src="/assets/img/blogs/tcp/mcp-correct-incorrect.png" width="950"/> </figure> <p><strong>Solution:</strong> True Class Probability is a more suitable confidence criteria</p> <h4 id="22-true-class-probability">2.2 True Class Probability</h4> <p>For a given input \(X_i\), the TCP refers to the probability corresponding to the ground-truth class, i.e.,</p> \[c^*_i = P(\hat{Y}_i=y_i^*|[\phi,\theta],X_i)\] <p>Here, \(c_i^*\) is the ground-truth confidence value for a particular input \(X_i\). This ensures that the model gives low confidence on the incorrectly predicted inputs and high confidence on the correctly predicted inputs. This is depicted from the following figure.</p> <figure style="text-align:center;"> <img src="/assets/img/blogs/tcp/tcp-correct-incorrect.png" width="950"/> </figure> <p>This directly quantifies the model confidence in the correct label, making it more informative for tasks such as active learning.</p> <h3 id="3-implementation">3. <strong>Implementation</strong></h3> <p>As we have seen, the True Class Probability is the predictive probability corresponding to the ground-truth class. But during inference, we do not have access to the ground-truth classes.</p> <p><strong>How to implement then?</strong></p> <p>We build an auxiliary confidence model \(f_{\psi}\) on top of the feature extractor layers \(\phi\) of the pre-trained base model. We use the same \(D_1^S\) to train such a model. With \(X_i\) as the input, the features are extracted from the feature extractor layers are fed to the confidence model that outputs a confidence prediction \(\hat{c}_i = f_{\psi}(X_i)\) which is a scalar value, i.e., \(\hat{c}_i \in [0,1]\). During training, the parameters \(\psi\) are learned such that the predicted confidence values are close to the actual or ground-truth confidence values \(c_i^*\). This is depicted from the following figure.</p> <figure style="text-align:center;"> <img src="/assets/img/blogs/tcp/tcp-model.png" width="450"/> </figure> <p>The parameters \(\psi\) are updated using gradient descent algorithm as:</p> \[\psi \leftarrow \psi - \beta \mathcal{L}_{conf}(f_{\psi})\] <p>where \(\beta \in \mathbb{R}^+\) is the learning rate and \(\mathcal{L}_{conf}\) is the mean squared loss defined as:</p> \[\mathcal{L}_{conf} = \frac{1}{I}\sum_{i=1}^{I}(\hat{c}_i - c_i^*)^2\] <h3 id="4-evaluation">4. <strong>Evaluation</strong></h3> <p>Consider an evaluation dataset \(D_2^S = \{(X_j)\}_{j=1}^J\) from the same domain. With \(X_j\) as the input, the pre-trained base model \(f_{[\phi,\theta]}\) gives the predicted class \(\hat{y}_j\) and the confidence model \(f_{\psi}\) gives the predicted confidence \(\hat{c}_i\in [0,1]\). If the confidence for a particular sample is low, the model flags the prediction as unreliable, indicating that the sample may be ambiguous, noisy, or close to the decision boundary. The predicted confidence values can be used to rank samples:</p> <ul> <li>Low-confidence samples are prioritized for human annotation or further inspection.</li> <li>High-confidence samples can be safely trusted or pseudo-labeled.</li> </ul> <p>This makes TCP particularly effective for active learning, where one selects samples with the lowest confidence for annotation.</p>]]></content><author><name></name></author><category term="Active Learning"/><category term="Machine Learning"/><category term="Confidence Estimation"/><summary type="html"><![CDATA[Mathematical formulation and theory of TCP.]]></summary></entry></feed>